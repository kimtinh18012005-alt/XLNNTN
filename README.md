
# Äá»€ CÆ¯Æ NG & Cáº¨M NANG THá»°C THI Dá»° ÃN MÃ”N Xá»¬ LÃ NGÃ”N NGá»® Tá»° NHIÃŠN #

**Äá» tÃ i: PhÃ¢n loáº¡i cÃ¢u há»i theo lÄ©nh vá»±c dá»±a trÃªn vÄƒn báº£n. Dataset: Yahoo Answers 10 categories for NLP (Kaggle). NhÃ³m thá»±c hiá»‡n (5 ngÆ°á»i): TÃ¬nh, Hiá»‡p, Nhá»±t Anh, BÃ¬nh, CÆ°á»ng.**

**PHáº¦N 1: SÆ¯á»œN BÃO CÃO WORD Tá»”NG THá»‚ (DÃ€N Ã BÃO CÃO)**

Pháº§n nÃ y lÃ  khung má»¥c lá»¥c cho file Word. CÃ¡c thÃ nh viÃªn bÃ¡m sÃ¡t vÃ o cÃ¡c má»¥c nÃ y Ä‘á»ƒ viáº¿t ná»™i dung tÆ°Æ¡ng á»©ng.

TRANG BÃŒA Lá»œI Cáº¢M Æ N TÃ“M Táº®T Äá»€ TÃ€I (ABSTRACT) Má»¤C Lá»¤C & DANH Má»¤C HÃŒNH áº¢NH, Báº¢NG BIá»‚U

**CHÆ¯Æ NG 1: Tá»”NG QUAN Äá»€ TÃ€I (NgÆ°á»i viáº¿t: TÃ¬nh)**

- 1.1 Äáº·t váº¥n Ä‘á» vÃ  tÃ­nh cáº¥p thiáº¿t (Táº¡i sao cáº§n phÃ¢n loáº¡i vÄƒn báº£n tá»± Ä‘á»™ng).
- 1.2 Má»¥c tiÃªu Ä‘á» tÃ i (PhÃ¢n loáº¡i tá»± Ä‘á»™ng 10 lÄ©nh vá»±c tá»« Yahoo Answers).
- 1.3 Äá»‘i tÆ°á»£ng vÃ  pháº¡m vi nghiÃªn cá»©u.
- 1.4 SÆ¡ Ä‘á»“ khá»‘i tá»•ng thá»ƒ cá»§a quy trÃ¬nh há»‡ thá»‘ng (Tá»« thu tháº­p data -> Tiá»n xá»­ lÃ½ -> Huáº¥n luyá»‡n -> ÄÃ¡nh giÃ¡).

**CHÆ¯Æ NG 2: CÆ  Sá» LÃ THUYáº¾T (NgÆ°á»i viáº¿t: Hiá»‡p, Nhá»±t Anh, BÃ¬nh)**

- 2.1 Tiá»n xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (Tokenization, Stopwords, Lemmatization) (Hiá»‡p).
- 2.2 TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vÄƒn báº£n (TF-IDF & Word Embedding) (Hiá»‡p & Nhá»±t Anh).
- 2.3 CÃ¡c thuáº­t toÃ¡n Machine Learning truyá»n thá»‘ng (Naive Bayes, SVM, Logistic Regression) (Hiá»‡p).
- 2.4 Máº¡ng há»c sÃ¢u chuá»—i (RNN, LSTM vÃ  cÆ¡ cháº¿ cá»•ng - Gates) (Nhá»±t Anh).
- 2.5 Kiáº¿n trÃºc Transformer vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (CÆ¡ cháº¿ Self-Attention, cáº¥u trÃºc máº¡ng BERT) (BÃ¬nh).

**CHÆ¯Æ NG 3: Dá»® LIá»†U VÃ€ TIá»€N Xá»¬ LÃ (NgÆ°á»i viáº¿t: TÃ¬nh)**

- 3.1 Nguá»“n gá»‘c vÃ  thá»‘ng kÃª táº­p dá»¯ liá»‡u Yahoo Answers.
- 3.2 PhÃ¢n tÃ­ch khÃ¡m phÃ¡ dá»¯ liá»‡u (EDA - WordCloud, Biá»ƒu Ä‘á»“ phÃ¢n phá»‘i nhÃ£n).
- 3.3 Quy trÃ¬nh lÃ m sáº¡ch dá»¯ liá»‡u (XÃ³a HTML, Regex kÃ½ tá»± Ä‘áº·c biá»‡t).
- 3.4 Chuáº©n hÃ³a vÃ  chuáº©n bá»‹ táº­p Train/Val/Test.

**CHÆ¯Æ NG 4: THá»°C NGHIá»†M VÃ€ HUáº¤N LUYá»†N MÃ” HÃŒNH (NgÆ°á»i viáº¿t: Hiá»‡p, Nhá»±t Anh, BÃ¬nh)**

- 4.1 MÃ´i trÆ°á»ng, cÃ´ng cá»¥ vÃ  thÆ° viá»‡n thá»±c nghiá»‡m (Pháº§n cá»©ng, Python, Sklearn, PyTorch).
- 4.2 Ká»‹ch báº£n 1: Huáº¥n luyá»‡n mÃ´ hÃ¬nh truyá»n thá»‘ng (Baseline) (Hiá»‡p).
- 4.3 Ká»‹ch báº£n 2: Huáº¥n luyá»‡n mÃ´ hÃ¬nh Deep Learning (Bi-LSTM) (Nhá»±t Anh).
- 4.4 Ká»‹ch báº£n 3: Tinh chá»‰nh mÃ´ hÃ¬nh Transformer (Fine-tuning BERT) (BÃ¬nh).

**CHÆ¯Æ NG 5: ÄÃNH GIÃ VÃ€ SO SÃNH (NgÆ°á»i viáº¿t: CÆ°á»ng)**

- 5.1 CÃ¡c thang Ä‘o Ä‘Ã¡nh giÃ¡ (CÃ´ng thá»©c Accuracy, Precision, Recall, F1-Score).
- 5.2 Káº¿t quáº£ thá»±c nghiá»‡m vÃ  báº£ng tá»•ng há»£p so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh.
- 5.3 PhÃ¢n tÃ­ch Ma tráº­n nháº§m láº«n (Confusion Matrix).
- 5.4 PhÃ¢n tÃ­ch lá»—i (Error Analysis - Táº¡i sao mÃ´ hÃ¬nh Ä‘oÃ¡n sai á»Ÿ má»™t sá»‘ cÃ¢u).

**CHÆ¯Æ NG 6: TRIá»‚N KHAI á»¨NG Dá»¤NG VÃ€ Káº¾T LUáº¬N (NgÆ°á»i viáº¿t: CÆ°á»ng)**

- 6.1 Giá»›i thiá»‡u cÃ´ng cá»¥ Streamlit vÃ  luá»“ng hoáº¡t Ä‘á»™ng cá»§a Web Demo.
- 6.2 Giao diá»‡n ngÆ°á»i dÃ¹ng vÃ  cÃ¡c vÃ­ dá»¥ cháº¡y thá»±c táº¿.
- 6.3 Káº¿t luáº­n chung.
- 6.4 Háº¡n cháº¿ vÃ  hÆ°á»›ng phÃ¡t triá»ƒn tÆ°Æ¡ng lai.

**TÃ€I LIá»†U THAM KHáº¢O**

**PHáº¦N 2: HÆ¯á»šNG DáºªN THá»°C THI CHI TIáº¾T (CÃCH LÃ€M CODE VÃ€ WORD)**

**1. TÃ¬nh (Data Engineer)**
- Viáº¿t Word: TrÃ¬nh bÃ y ChÆ°Æ¡ng 1 vÃ  ChÆ°Æ¡ng 3. Pháº£i chá»¥p hÃ¬nh báº£ng dá»¯ liá»‡u gá»‘c (nhiá»u rÃ¡c) vÃ  báº£ng dá»¯ liá»‡u sau khi lÃ m sáº¡ch Ä‘á»ƒ Ä‘Æ°a vÃ o bÃ¡o cÃ¡o lÃ m báº±ng chá»©ng. Váº½ biá»ƒu Ä‘á»“ WordCloud cho 2-3 nhÃ£n Ä‘á»ƒ lÃ m sinh Ä‘á»™ng.
- Ná»n táº£ng code & cháº¡y: DÃ¹ng Jupyter Notebook (cháº¡y trÃªn mÃ¡y cÃ¡ nhÃ¢n/Local) hoáº·c Google Colab. VÃ¬ xá»­ lÃ½ text báº±ng Pandas vÃ  Regex chá»‰ dÃ¹ng CPU, nÃªn cháº¡y á»Ÿ Ä‘Ã¢u cÅ©ng Ä‘Æ°á»£c. Náº¿u data quÃ¡ lá»›n (vÃ i trÄƒm nghÃ¬n dÃ²ng) thÃ¬ nÃªn nÃ©m lÃªn Google Colab cho nhanh.
- Code: \* DÃ¹ng  pandas gá»™p cá»™t  Question Title vÃ   Question Content .
  - DÃ¹ng thÆ° viá»‡n  re Ä‘á»ƒ loáº¡i bá» kÃ½ tá»± láº¡.
  - DÃ¹ng  nltk Ä‘á»ƒ xÃ³a stopwords.
  - DÃ¹ng  WordNetLemmatizer() Ä‘á»ƒ Ä‘Æ°a tá»« vá» nguyÃªn máº«u.
  - Xuáº¥t file  clean\_data.csv gá»­i cho cáº£ nhÃ³m.
 
**2. Hiá»‡p (Traditional ML Engineer)**
- Viáº¿t Word: Viáº¿t pháº§n 2.1, 2.2, 2.3 vÃ  4.2. TrÃ¬nh bÃ y cÃ´ng thá»©c toÃ¡n há»c cá»§a TF-IDF vÃ  Ä‘á»‹nh lÃ½ Naive Bayes.
- Ná»n táº£ng code & cháº¡y: DÃ¹ng Google Colab hoáº·c Jupyter Notebook. CÃ¡c model Machine Learning truyá»n thá»‘ng (SVM, Naive Bayes) cháº¡y ráº¥t nháº¹, CPU mÃ¡y tÃ­nh thÃ´ng thÆ°á»ng (Core i5, RAM 8GB) váº«n cháº¡y mÆ°á»£t mÃ .
- Code: \* Äá»c  clean\_data.csv .
  - DÃ¹ng  TfidfVectorizer(ngram\_range=(1,2)) chuyá»ƒn text thÃ nh ma tráº­n sá»‘.
  - Fit dá»¯ liá»‡u vÃ o 3 model  MultinomialNB() ,  LogisticRegression() ,  LinearSVC() .
  - LÆ°u model SVC/Logistic tá»‘t nháº¥t thÃ nh file  .pkl Ä‘á»ƒ gá»­i cho CÆ°á»ng lÃ m Demo.
 
**3. Nhá»±t Anh (Deep Learning Engineer)**
- Viáº¿t Word: Viáº¿t pháº§n 2.4 vÃ  4.3. Báº¯t buá»™c chÃ¨n hÃ¬nh áº£nh sÆ¡ trÃºc táº¿ bÃ o LSTM (cÃ³ cÃ¡c cá»•ng Input, Forget, Output).
- Ná»n táº£ng code & cháº¡y: Báº¯t buá»™c dÃ¹ng Google Colab. Pháº£i vÃ o má»¥c Runtime -> Change runtime type -> Chá»n T4 GPU. MÃ´ hÃ¬nh máº¡ng NÆ¡-ron xá»­ lÃ½ hÃ ng triá»‡u tham sá»‘, náº¿u cháº¡y báº±ng CPU mÃ¡y tÃ­nh bÃ n sáº½ máº¥t vÃ i ngÃ y, dÃ¹ng GPU cá»§a Colab chá»‰ máº¥t khoáº£ng 15-30 phÃºt.
- Code: \* DÃ¹ng  Tokenizer cá»§a Keras Ä‘á»ƒ chuyá»ƒn tá»« vá»±ng thÃ nh index.
  - DÃ¹ng  pad\_sequences giá»›i háº¡n Ä‘á»™ dÃ i cÃ¢u (vd: 150 tá»«).
  - Build model vá»›i lá»›p  Embedding ->  Bidirectional(LSTM(64)) ->  Dense(10,

    activation='softmax') .

    1. Train trÃªn Google Colab GPU. Váº½ Ä‘á»“ thá»‹ Loss/Accuracy qua cÃ¡c Epochs.
  
**4. BÃ¬nh (Advanced NLP Engineer)**
   1. Viáº¿t Word: Viáº¿t pháº§n 2.5 vÃ  4.4. ÄÆ°a hÃ¬nh áº£nh kiáº¿n trÃºc Transformer (pháº§n Encoder) vÃ  giáº£i thÃ­ch cÆ¡ cháº¿ Self-Attention.
   1. Ná»n táº£ng code & cháº¡y: Báº¯t buá»™c dÃ¹ng Google Colab (GPU) hoáº·c Kaggle Notebook. Model BERT lÃ  model cá»±c náº·ng (HÃ ng trÄƒm triá»‡u tham sá»‘), yÃªu cáº§u GPU pháº£i cÃ³ VRAM lá»›n (nhÆ° GPU T4 16GB VRAM trÃªn Colab). KhÃ´ng Ä‘Æ°á»£c cháº¡y trÃªn laptop cÃ¡ nhÃ¢n vÃ¬ sáº½ gÃ¢y trÃ n RAM vÃ  treo mÃ¡y.
   1. Code: \* NÃªn dÃ¹ng  DistilBERT thay vÃ¬ BERT gá»‘c Ä‘á»ƒ cháº¡y nhanh hÆ¡n (chá»‰ máº¥t ná»­a thá»i gian train).
      1. DÃ¹ng  AutoTokenizer vÃ   AutoModelForSequenceClassification cá»§a thÆ° viá»‡n transformers (Hugging Face).
      1. CÃ i Ä‘áº·t  TrainingArguments vÃ  dÃ¹ng class  Trainer Ä‘á»ƒ fine-tuning. LÆ°u láº¡i log Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ so sÃ¡nh.

**5. CÆ°á»ng (Evaluation & Deployment)**

- Viáº¿t Word: Viáº¿t ChÆ°Æ¡ng 5 vÃ  ChÆ°Æ¡ng 6. Láº¥y káº¿t quáº£ (F1-Score) tá»« Hiá»‡p, Nhá»±t Anh, BÃ¬nh Ä‘á»ƒ káº» báº£ng so sÃ¡nh. Chá»¥p giao diá»‡n Web chÃ¨n vÃ o Word.
- Ná»n táº£ng code & cháº¡y: DÃ¹ng Visual Studio Code (VS Code) hoáº·c PyCharm cháº¡y trá»±c tiáº¿p trÃªn Laptop cÃ¡ nhÃ¢n (Local). Viá»‡c lÃ m Web báº±ng Streamlit cáº§n má»Ÿ trÃ¬nh duyá»‡t Localhost Ä‘á»ƒ xem trá»±c tiáº¿p, nÃªn code á»Ÿ mÃ¡y cÃ¡ nhÃ¢n sáº½ dá»… nhÃ¬n giao diá»‡n vÃ  debug nháº¥t. KhÃ´ng nÃªn code pháº§n nÃ y trÃªn Colab.
- Code: \* Nháº­n file  .pkl tá»« Hiá»‡p.
  - Viáº¿t giao diá»‡n Web báº±ng  Streamlit (xem hÆ°á»›ng dáº«n á»Ÿ Pháº§n 4).
  - Váº½ Confusion Matrix báº±ng thÆ° viá»‡n  seaborn.heatmap .

**PHáº¦N 3: YÃŠU Cáº¦U KIáº¾N THá»¨C Báº®T BUá»˜C (Äá»‚ Báº¢O Vá»† Äá»’ ÃN)**

ÄÃ¢y lÃ  nhá»¯ng cÃ¢u há»i giáº£ng viÃªn cháº¯c cháº¯n sáº½ há»i. NgÆ°á»i nÃ o phá»¥ trÃ¡ch pháº§n nÃ o thÃ¬ PHáº¢I náº¯m vá»¯ng kiáº¿n thá»©c pháº§n Ä‘Ã³.

**Kiáº¿n thá»©c TÃ¬nh pháº£i náº¯m:**

1. Táº¡i sao pháº£i xÃ³a Stopwords? (Tráº£ lá»i: Äá»ƒ giáº£m chiá»u dá»¯ liá»‡u, tiáº¿t kiá»‡m RAM. CÃ¡c tá»« nhÆ° "the, is, and" xuáº¥t hiá»‡n nhiá»u nhÆ°ng khÃ´ng mang Ã½ nghÄ©a phÃ¢n loáº¡i chuyÃªn má»¥c).
1. Stemming vÃ  Lemmatization khÃ¡c nhau tháº¿ nÃ o? Táº¡i sao nhÃ³m chá»n Lemmatization? (Tráº£ lá»i: Stemming chá»‰ cáº¯t Ä‘uÃ´i tá»« má»™t cÃ¡ch mÃ¡y mÃ³c (vÃ­ dá»¥: "caring" -> "car"), dá»… lÃ m sai nghÄ©a. Lemmatization dÃ¹ng tá»« Ä‘iá»ƒn Ä‘á»ƒ Ä‘Æ°a vá» tá»« gá»‘c Ä‘Ãºng ngá»¯ phÃ¡p (vÃ­ dá»¥: "better" -> "good"). NhÃ³m chá»n Lemma vÃ¬ nÃ³ giá»¯ nguyÃªn ngá»¯ nghÄ©a).
1. Imbalanced Data (Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng) lÃ  gÃ¬? Táº­p Yahoo cÃ³ bá»‹ máº¥t cÃ¢n báº±ng khÃ´ng? (Cáº§n má»Ÿ data xem má»—i nhÃ£n cÃ³ sá»‘ lÆ°á»£ng cÃ¢u há»i báº±ng nhau khÃ´ng).

**Kiáº¿n thá»©c Hiá»‡p pháº£i náº¯m:**

1. Giáº£i thÃ­ch cÃ´ng thá»©c TF-IDF? Táº¡i sao nÃ³ tá»‘t hÆ¡n Bag-of-Words (BoW) Ä‘áº¿m táº§n suáº¥t Ä‘Æ¡n thuáº§n? (Tráº£ lá»i: TF-IDF pháº¡t nhá»¯ng tá»« xuáº¥t hiá»‡n quÃ¡ nhiá»u á»Ÿ Má»ŒI vÄƒn báº£n (nhÆ° tá»« "question"). Má»™t tá»« cÃ³ TF-IDF cao nghÄ©a lÃ  nÃ³ xuáº¥t hiá»‡n nhiá»u trong vÄƒn báº£n Ä‘Ã³, nhÆ°ng hiáº¿m gáº·p á»Ÿ cÃ¡c vÄƒn báº£n khÃ¡c -> NÃ³ lÃ  Ä‘áº·c trÆ°ng riÃªng biá»‡t cá»§a lÄ©nh vá»±c Ä‘Ã³).
1. Táº¡i sao Naive Bayes láº¡i gá»i lÃ  "Naive" (NgÃ¢y thÆ¡)? (Tráº£ lá»i: VÃ¬ nÃ³ giáº£ Ä‘á»‹nh cÃ¡c tá»« trong cÃ¢u xuáº¥t hiá»‡n Ä‘á»™c láº­p vá»›i nhau. Trong thá»±c táº¿, cÃ¡c tá»« luÃ´n cÃ³ ngá»¯ cáº£nh liÃªn káº¿t, nhÆ°ng giáº£ Ä‘á»‹nh ngÃ¢y thÆ¡ nÃ y váº«n lÃ m viá»‡c ráº¥t tá»‘t cho Text Classification).
1. N-gram lÃ  gÃ¬? (Tráº£ lá»i: Viá»‡c dÃ¹ng  ngram\_range=(1,2) giÃºp model báº¯t Ä‘Æ°á»£c cá»¥m 2 tá»« cÃ³ nghÄ©a, vÃ­ dá»¥ "New York" thay vÃ¬ 2 tá»« rá»i ráº¡c "New" vÃ  "York").

**Kiáº¿n thá»©c Nhá»±t Anh pháº£i náº¯m:**

1. Khuyáº¿t Ä‘iá»ƒm cá»§a RNN lÃ  gÃ¬ mÃ  pháº£i dÃ¹ng LSTM? (Tráº£ lá»i: RNN bá»‹ há»™i chá»©ng "Triá»‡t tiÃªu Ä‘áº¡o hÃ m" (Vanishing Gradient). Khi cÃ¢u quÃ¡ dÃ i, RNN sáº½ "quÃªn" máº¥t nhá»¯ng tá»« á»Ÿ Ä‘áº§u cÃ¢u. LSTM cÃ³ cÃ¡c Cá»•ng (Gates) bá»™ nhá»› giÃºp nÃ³ quyáº¿t Ä‘á»‹nh nÃªn nhá»› hay quÃªn thÃ´ng tin nÃ o, giáº£i quyáº¿t cÃ¢u dÃ i ráº¥t tá»‘t).
1. Word Embedding (Lá»›p nhÃºng tá»«) lÃ  gÃ¬? (Tráº£ lá»i: KhÃ¡c vá»›i TF-IDF lÃ  ma tráº­n thÆ°a thá»›t (toÃ n sá»‘ 0), Embedding chuyá»ƒn má»—i tá»« thÃ nh má»™t vector máº­t Ä‘á»™ cao (dense vector) trong khÃ´ng gian n-chiá»u. CÃ¡c tá»« cÃ¹ng nghÄ©a (vÃ­ dá»¥ "king" vÃ  "queen") sáº½ cÃ³ vector náº±m gáº§n nhau trong khÃ´ng gian).
1. Táº¡i sao láº¡i dÃ¹ng Bi-LSTM (Bidirectional) thay vÃ¬ LSTM thÆ°á»ng? (Tráº£ lá»i: Bi-LSTM Ä‘á»c cÃ¢u vÄƒn theo 2 chiá»u: tá»« trÃ¡i sang pháº£i vÃ  tá»« pháº£i sang trÃ¡i. Äiá»u nÃ y giÃºp model hiá»ƒu toÃ n bá»™ ngá»¯ cáº£nh xung quanh má»™t tá»«).

**Kiáº¿n thá»©c BÃ¬nh pháº£i náº¯m:**

1. CÆ¡ cháº¿ Self-Attention lÃ  gÃ¬? (Tráº£ lá»i: LÃ  cÆ¡ cháº¿ giÃºp mÃ´ hÃ¬nh khi Ä‘ang Ä‘á»c má»™t tá»«, cÃ³ thá»ƒ "chÃº Ã½" (attend) Ä‘áº¿n cÃ¡c tá»« quan trá»ng khÃ¡c trong CÃ™NG cÃ¢u Ä‘Ã³, báº¥t ká»ƒ khoáº£ng cÃ¡ch xa gáº§n, Ä‘á»ƒ hiá»ƒu rÃµ nghÄ©a cá»§a tá»« Ä‘ang Ä‘á»c).
1. Táº¡i sao dÃ¹ng Transfer Learning (Fine-tune BERT) láº¡i tá»‘t hÆ¡n tá»± train model tá»« Ä‘áº§u? (Tráº£ lá»i: BERT Ä‘Ã£ Ä‘Æ°á»£c Google train sáºµn trÃªn toÃ n bá»™ Wikipedia (hÃ ng tá»· tá»«) nÃªn nÃ³ "hiá»ƒu" ngá»¯ phÃ¡p tiáº¿ng Anh cá»±c ká»³ sÃ¢u. MÃ¬nh chá»‰ cáº§n Ä‘em vá» huáº¥n luyá»‡n thÃªm má»™t lá»›p nhá» (fine-tune) trÃªn táº­p Yahoo Answers lÃ  Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t).
1. Háº¡n cháº¿ lá»›n nháº¥t cá»§a BERT lÃ  gÃ¬? (Tráº£ lá»i: Model ráº¥t náº·ng, cáº§n GPU xá»‹n Ä‘á»ƒ train, vÃ  thá»i gian dá»± Ä‘oÃ¡n (inference) cháº­m hÆ¡n so vá»›i Naive Bayes hay SVM).

**Kiáº¿n thá»©c CÆ°á»ng pháº£i náº¯m:**

1. Precision, Recall lÃ  gÃ¬? Táº¡i sao F1-Score láº¡i quan trá»ng hÆ¡n Accuracy? \* Precision: ÄoÃ¡n lÃ  A, thÃ¬ trÃºng A bao nhiÃªu %.
- Recall: Trong tá»•ng sá»‘ thá»±c táº¿ lÃ  A, model tÃ¬m ra Ä‘Æ°á»£c bao nhiÃªu %.
- F1-Score: LÃ  trung bÃ¬nh Ä‘iá»u hÃ²a cá»§a Precision vÃ  Recall. Náº¿u dá»¯ liá»‡u bá»‹ lá»‡ch (nhÃ£n nÃ y nhiá»u hÆ¡n nhÃ£n kia), Accuracy sáº½ Ä‘Ã¡nh lá»«a ta, lÃºc nÃ y báº¯t buá»™c pháº£i nhÃ¬n vÃ o F1-Score.
2. Äá»c Confusion Matrix tháº¿ nÃ o? (Tráº£ lá»i: ÄÆ°á»ng chÃ©o chÃ­nh lÃ  sá»‘ lÆ°á»£ng Ä‘oÃ¡n Ä‘Ãºng. CÃ¡c Ã´ náº±m ngoÃ i Ä‘Æ°á»ng chÃ©o lÃ  model Ä‘oÃ¡n sai. NhÃ¬n vÃ o Ä‘Ã³ Ä‘á»ƒ biáº¿t model hay nháº§m láº«n giá»¯a LÄ©nh vá»±c A vÃ  LÄ©nh vá»±c B).
2. LÃ m sao Ä‘á»ƒ lÆ°u model vÃ  Ä‘Æ°a lÃªn Web? (Tráº£ lá»i: DÃ¹ng  joblib hoáº·c  pickle lÆ°u cáº¥u trÃºc máº¡ng vÃ  tham sá»‘ thÃ nh file. Khi Web cháº¡y, nÃ³ load file nÃ y lÃªn RAM vÃ  gá»i hÃ m model.predict() ).

**PHáº¦N 4: HÆ¯á»šNG DáºªN CODE WEB DEMO CHO CÆ¯á»œNG (STREAMLIT)**

BÆ°á»›c 1: CÃ i Ä‘áº·t (Má»Ÿ terminal/CMD cá»§a mÃ¡y tÃ­nh cÃ¡ nhÃ¢n gÃµ)  pip install streamlit scikit - learn

BÆ°á»›c 2: Chuáº©n bá»‹ file Láº¥y file  traditional\_model.pkl vÃ   tfidf\_vectorizer.pkl (tá»« báº¡n Hiá»‡p) Ä‘á»ƒ chung thÆ° má»¥c.

BÆ°á»›c 3: Táº¡o file  app.py vá»›i Ä‘oáº¡n code siÃªu ngáº¯n gá»n nÃ y (DÃ¹ng VS Code Ä‘á»ƒ soáº¡n tháº£o):

import streamlit as st import joblib

- Load Model

vectorizer = joblib.load('tfidf\_vectorizer.pkl') model = joblib.load('traditional\_model.pkl')

classes = ['Society', 'Science', 'Health', 'Education', 'Computers', 

`           `'Sports', 'Business', 'Entertainment', 'Family', 'Politics']

- Giao diá»‡n

st.title("ğŸ¤– Demo PhÃ¢n Loáº¡i Chá»§ Äá» Yahoo Answers")

user\_input = st.text\_area("Nháº­p cÃ¢u há»i tiáº¿ng Anh vÃ o Ä‘Ã¢y:", height=100)

if st.button("Dá»± Ä‘oÃ¡n"):

`    `if user\_input:

`        `vec\_text = vectorizer.transform([user\_input])

`        `pred = model.predict(vec\_text)

`        `st.success(f"ğŸ¯ Káº¿t quáº£: ÄÃ¢y lÃ  lÄ©nh vá»±c \*\*{classes[pred[0]]}\*\*")         st.balloons()

`    `else:

`        `st.warning("Vui lÃ²ng nháº­p vÄƒn báº£n!")

BÆ°á»›c 4: Cháº¡y Demo Má»Ÿ Terminal trong VS Code (Ctrl + ~), gÃµ lá»‡nh  streamlit run app.py . TrÃ¬nh duyá»‡t cá»§a mÃ¡y tÃ­nh sáº½ tá»± má»Ÿ ra trang web cá»±c xá»‹n sÃ² á»Ÿ Ä‘á»‹a chá»‰  localhost:8501 Ä‘á»ƒ báº¡n biá»ƒu diá»…n trá»±c tiáº¿p lÃºc lÃªn bÃ¡o cÃ¡o.

**PHáº¦N 5: Báº¢N MáºªU HOÃ€N THIá»†N BÃO CÃO (DRAFT TEMPLATE)**

(Ghi chÃº: NhÃ³m dÃ¹ng pháº§n nÃ y lÃ m vÄƒn máº«u há»c thuáº­t Ä‘á»ƒ copy/paste vÃ o file Word, sau Ä‘Ã³ tá»± Ä‘iá»n thÃªm hÃ¬nh áº£nh vÃ  sá»‘ liá»‡u thá»±c táº¿)

**TÃ“M Táº®T Äá»€ TÃ€I (ABSTRACT)**

Trong ká»· nguyÃªn bÃ¹ng ná»• thÃ´ng tin, viá»‡c tá»± Ä‘á»™ng hÃ³a quÃ¡ trÃ¬nh phÃ¢n loáº¡i vÄƒn báº£n Ä‘Ã³ng vai trÃ² cá»‘t lÃµi trong cÃ¡c há»‡ thá»‘ng há»i Ä‘Ã¡p (Q&A) vÃ  Ä‘á»‹nh tuyáº¿n thÃ´ng tin. Äá»“ Ã¡n nÃ y táº­p trung giáº£i quyáº¿t bÃ i toÃ¡n phÃ¢n loáº¡i cÃ¢u há»i tiáº¿ng Anh vÃ o 10 lÄ©nh vá»±c khÃ¡c nhau sá»­ dá»¥ng táº­p dá»¯ liá»‡u Yahoo Answers. NhÃ³m nghiÃªn cá»©u Ä‘Ã£ thá»±c hiá»‡n má»™t quy trÃ¬nh toÃ n diá»‡n tá»« bÆ°á»›c tiá»n xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vÄƒn báº£n, cho Ä‘áº¿n viá»‡c Ã¡p dá»¥ng vÃ  so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh tá»« cÆ¡ báº£n Ä‘áº¿n

hiá»‡n Ä‘áº¡i. Cá»¥ thá»ƒ, bÃ¡o cÃ¡o tiáº¿n hÃ nh Ä‘á»‘i chiáº¿u hiá»‡u nÄƒng giá»¯a mÃ´ hÃ¬nh Há»c mÃ¡y truyá»n thá»‘ng (SVM, Naive Bayes), mÃ´ hÃ¬nh Há»c sÃ¢u (Bi-LSTM) vÃ  kiáº¿n trÃºc tiÃªn tiáº¿n Transformer (DistilBERT). Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y mÃ´ hÃ¬nh Ä‘iá»n tÃªn model tá»‘t nháº¥t,VD : DistilBERT Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t vá»›i F1-Score lÃªn tá»›i : Ä‘iá»n sá»‘ vÃ o ,VD : 76.5

. Cuá»‘i cÃ¹ng, nhÃ³m Ä‘Ã£ triá»ƒn khai thÃ nh cÃ´ng má»™t á»©ng dá»¥ng Web Demo trá»±c quan hÃ³a káº¿t quáº£ cá»§a mÃ´ hÃ¬nh trong Ä‘iá»u kiá»‡n thá»±c táº¿.

**CHÆ¯Æ NG 1: Tá»”NG QUAN Äá»€ TÃ€I**

1. Äáº·t váº¥n Ä‘á» HÃ ng ngÃ y, cÃ¡c diá»…n Ä‘Ã n há»i Ä‘Ã¡p nhÆ° Yahoo Answers, Quora hay Reddit nháº­n Ä‘Æ°á»£c hÃ ng triá»‡u cÃ¢u há»i má»›i. Náº¿u sá»­ dá»¥ng sá»©c ngÆ°á»i Ä‘á»ƒ Ä‘á»c vÃ  phÃ¢n loáº¡i tá»«ng cÃ¢u há»i vÃ o Ä‘Ãºng chuyÃªn má»¥c (Thá»ƒ thao, CÃ´ng nghá»‡, Sá»©c khá»e...) lÃ  Ä‘iá»u báº¥t kháº£ thi. Sá»± phÃ¢n loáº¡i thiáº¿u chÃ­nh xÃ¡c sáº½ khiáº¿n ngÆ°á»i dÃ¹ng khÃ´ng tiáº¿p cáº­n Ä‘Æ°á»£c cÃ¡c chuyÃªn gia tÆ°Æ¡ng á»©ng, lÃ m giáº£m cháº¥t lÆ°á»£ng cá»§a ná»n táº£ng. Do Ä‘Ã³, bÃ i toÃ¡n Text Classification (PhÃ¢n loáº¡i vÄƒn báº£n) á»©ng dá»¥ng NLP trá»Ÿ nÃªn vÃ´ cÃ¹ng cáº¥p thiáº¿t.
1. Má»¥c tiÃªu nghiÃªn cá»©u XÃ¢y dá»±ng má»™t quy trÃ¬nh Pipeline hoÃ n chá»‰nh Ä‘á»ƒ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn tiáº¿ng Anh. ÄÃ¡nh giÃ¡ vÃ  tÃ¬m ra mÃ´ hÃ¬nh há»c mÃ¡y tá»‘i Æ°u nháº¥t Ä‘á»ƒ phÃ¢n loáº¡i tá»± Ä‘á»™ng má»™t vÄƒn báº£n (bao gá»“m tiÃªu Ä‘á» vÃ  ná»™i dung) vÃ o Ä‘Ãºng 1 trong 10 nhÃ£n danh má»¥c cá»§a Yahoo Answers.

   **CHÆ¯Æ NG 2: CÆ  Sá» LÃ THUYáº¾T**

1. PhÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (TF-IDF) Äá»ƒ mÃ¡y tÃ­nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c ngÃ´n ngá»¯ cá»§a con ngÆ°á»i, vÄƒn báº£n cáº§n Ä‘Æ°á»£c sá»‘ hÃ³a. NhÃ³m sá»­ dá»¥ng ká»¹ thuáº­t TF-IDF (Term Frequency - Inverse Document Frequency). KhÃ¡c vá»›i viá»‡c Ä‘áº¿m táº§n suáº¥t Ä‘Æ¡n thuáº§n, TF-IDF Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ quan trá»ng cá»§a má»™t tá»« thÃ´ng qua viá»‡c pháº¡t trá»ng sá»‘ cá»§a nhá»¯ng tá»« xuáº¥t hiá»‡n quÃ¡ phá»• biáº¿n á»Ÿ má»i tÃ i liá»‡u (nhÆ° "the", "a", "is"). CÃ´ng thá»©c: : TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)
1. Máº¡ng NÆ¡-ron há»“i quy vÃ  LSTM Äá»‘i vá»›i vÄƒn báº£n, thá»© tá»± xuáº¥t hiá»‡n cá»§a tá»« ngá»¯ mang Ã½ nghÄ©a quyáº¿t Ä‘á»‹nh. Máº¡ng RNN truyá»n thá»‘ng gáº·p pháº£i hiá»‡n tÆ°á»£ng triá»‡t tiÃªu Ä‘áº¡o hÃ m (Vanishing Gradient) khi xá»­ lÃ½ cÃ¢u dÃ i. Thuáº­t toÃ¡n LSTM ra Ä‘á»i vá»›i cÆ¡ cháº¿ "Táº¿ bÃ o bá»™ nhá»›" cÃ¹ng 3 cá»•ng (Input, Forget, Output Gate) giÃºp mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng ghi nhá»› cÃ¡c thÃ´ng tin ngá»¯ cáº£nh quan trá»ng tá»« Ä‘áº§u cÃ¢u Ä‘áº¿n cuá»‘i cÃ¢u.
   CHÃˆN HÃŒNH áº¢NH SÆ  Äá»’ Máº NG LSTM

**CHÆ¯Æ NG 3: Dá»® LIá»†U VÃ€ TIá»€N Xá»¬ LÃ**

1. PhÃ¢n tÃ­ch táº­p dá»¯ liá»‡u (EDA) Táº­p dá»¯ liá»‡u Yahoo Answers gá»“m 10 danh má»¥c (Society, Science, Health...). NhÃ¬n vÃ o biá»ƒu Ä‘á»“ phÃ¢n phá»‘i nhÃ£n bÃªn dÆ°á»›i, ta tháº¥y sá»‘ lÆ°á»£ng máº«u á»Ÿ má»—i lá»›p lÃ  khÃ¡ Ä‘á»“ng Ä‘á»u, do Ä‘Ã³ nhÃ³m khÃ´ng cáº§n Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t cÃ¢n báº±ng dá»¯ liá»‡u phá»©c táº¡p (nhÆ° SMOTE).

CHÃˆN HÃŒNH BACHAR Äáº¾M Sá» LÆ¯á»¢NG Tá»ªNG CLASS 

2. Tiá»n xá»­ lÃ½ (Data Preprocessing) VÄƒn báº£n gá»‘c do ngÆ°á»i dÃ¹ng máº¡ng nháº­p chá»©a ráº¥t nhiá»u lá»—i. NhÃ³m Ä‘Ã£ thá»±c hiá»‡n cÃ¡c bÆ°á»›c chuáº©n hÃ³a tuáº§n tá»±:
- Lá»c nhiá»…u: XÃ³a tháº»  <br> , URL vÃ  cÃ¡c icon khÃ´ng mang Ã½ nghÄ©a ngÃ´n ngá»¯.
- Stopwords Removal: Loáº¡i bá» cÃ¡c giá»›i tá»«, liÃªn tá»« tiáº¿ng Anh thÃ´ng qua thÆ° viá»‡n NLTK.
- Lemmatization: Chuyá»ƒn Ä‘á»•i cÃ¡c tá»« vá» dáº¡ng nguyÃªn thá»ƒ cÆ¡ sá»Ÿ (vÃ­ dá»¥: "better" -> "good", "running" -> "run") nháº±m thu gá»n kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn.

CHÃˆN Báº¢NG : 1 Cá»˜T CÃ‚U Gá»C VÃ€ 1 CÃ‚U SAU KHI Xá»¬ LÃ 

**CHÆ¯Æ NG 4: THá»°C NGHIá»†M VÃ€ HUáº¤N LUYá»†N**

1. MÃ´i trÆ°á»ng thá»±c nghiá»‡m CÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn ná»n táº£ng Ä‘Ã¡m mÃ¢y Google Colab vá»›i cáº¥u hÃ¬nh GPU Tesla T4 16GB, RAM 12GB. NgÃ´n ngá»¯ láº­p trÃ¬nh lÃ  Python 3.10. Má»™t sá»‘ mÃ´ hÃ¬nh cÆ¡ sá»Ÿ nháº¹ vÃ  á»©ng dá»¥ng Web Ä‘Æ°á»£c cháº¡y thá»­ nghiá»‡m trá»±c tiáº¿p trÃªn mÃ¡y tÃ­nh cÃ¡ nhÃ¢n (Local) thÃ´ng qua Visual Studio Code.
1. QuÃ¡ trÃ¬nh huáº¥n luyá»‡n
- Äá»‘i vá»›i SVM: NhÃ³m sá»­ dá»¥ng ma tráº­n TF-IDF vá»›i n-gram = (1, 2) (láº¥y cáº£ tá»« Ä‘Æ¡n vÃ  cá»¥m 2 tá»«). SiÃªu tham sá»‘ pháº¡t C Ä‘Æ°á»£c Ä‘áº·t báº±ng 1.0 vá»›i kernel Linear.
- Äá»‘i vá»›i BERT: NhÃ³m sá»­ dá»¥ng kiáº¿n trÃºc DistilBERT nháº±m tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½. QuÃ¡ trÃ¬nh Fine- tuning diá»…n ra trong 3 Epochs, hÃ m tá»‘i Æ°u lÃ  AdamW vá»›i Learning Rate khá»Ÿi táº¡o lÃ  2e-5, Batch size báº±ng 16.

**CHÆ¯Æ NG 5: ÄÃNH GIÃ VÃ€ SO SÃNH**

**1. Káº¿t quáº£ thá»±c nghiá»‡m DÆ°á»›i Ä‘Ã¢y lÃ  báº£ng tá»•ng há»£p hiá»‡u nÄƒng cá»§a 3 nhÃ³m phÆ°Æ¡ng phÃ¡p chÃ­nh dá»±a trÃªn táº­p kiá»ƒm thá»­ (Test set).**

51\. Ket qua thutc nghiÃ©m DuÂ¢i day la bang tong hop hiÃ©u nang cia 3 nhom phvong phap chinh dua trÃ©n

KÃ©t qua So sanh M6hinh
MÃ´ hÃ¬nh,Äáº·c trÆ°ng (Features),Accuracy,Precision,Recall,F1-Score
Naive Bayes,TF-IDF,68.2%,68.5%,68.2%,68.1%
Linear SVM,TF-IDF,72.4%,72.6%,72.4%,72.5%
Bi-LSTM,Word2Vec / Embedding,73.8%,73.5%,73.8%,73.6%
DistilBERT,Transformer,77.1%,77.3%,77.1%,77.2%


Nháº­n xÃ©t: ÄÃºng nhÆ° dá»± Ä‘oÃ¡n lÃ½ thuyáº¿t, mÃ´ hÃ¬nh Transformer (DistilBERT) Ã¡p Ä‘áº£o hoÃ n toÃ n cÃ¡c mÃ´ hÃ¬nh tiá»n nhiá»‡m. Kháº£ nÄƒng tháº¥u hiá»ƒu ngá»¯ cáº£nh hai chiá»u nhá» cÆ¡ cháº¿ Attention giÃºp BERT phÃ¢n loáº¡i xuáº¥t sáº¯c cÃ¡c cÃ¢u há»i chá»©a cÃ¡c tá»« cÃ³ nghÄ©a Ä‘a chiá»u.

**2. PhÃ¢n tÃ­ch lá»—i (Error Analysis) thÃ´ng qua Confusion Matrix**
Dá»±a vÃ o ma tráº­n nháº§m láº«n, mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t á»Ÿ lÄ©nh vá»±c "Sports" vÃ  "Computers". Tuy nhiÃªn, mÃ´ hÃ¬nh cÃ³ xu hÆ°á»›ng nháº§m láº«n cao giá»¯a hai lá»›p "Science & Mathematics" vÃ  "Computers & Internet". NguyÃªn nhÃ¢n do hai lÄ©nh vá»±c nÃ y chia sáº» quÃ¡ nhiá»u thuáº­t ngá»¯ chuyÃªn ngÃ nh chung (vÃ­ dá»¥: data, calculation, formula...).

**CHÆ¯Æ NG 6: TRIá»‚N KHAI VÃ€ Káº¾T LUáº¬N**

1. Triá»ƒn khai Web Demo Nháº±m minh há»a tÃ­nh á»©ng dá»¥ng cá»§a Ä‘á» tÃ i, nhÃ³m Ä‘Ã£ sá»­ dá»¥ng thÆ° viá»‡n Streamlit Ä‘Ã³ng gÃ³i mÃ´ hÃ¬nh SVM (do tÃ­nh gá»n nháº¹ vÃ  tá»‘c Ä‘á»™ dá»± Ä‘oÃ¡n tá»©c thÃ¬ trÃªn mÃ¡y tÃ­nh cÃ¡ nhÃ¢n) thÃ nh má»™t á»©ng dá»¥ng Web trá»±c quan. NgÆ°á»i dÃ¹ng nháº­p cÃ¢u há»i vÃ o Ã´ vÄƒn báº£n, há»‡ thá»‘ng sáº½ tiá»n xá»­ lÃ½ ngáº§m, mÃ£ hÃ³a vector vÃ  tráº£ vá» káº¿t quáº£ dá»± Ä‘oÃ¡n trÃªn trÃ¬nh duyá»‡t Localhost.
2. Káº¿t luáº­n Äá»“ Ã¡n Ä‘Ã£ hoÃ n thÃ nh xuáº¥t sáº¯c má»¥c tiÃªu ban Ä‘áº§u: XÃ¢y dá»±ng thÃ nh cÃ´ng há»‡ thá»‘ng phÃ¢n loáº¡i cÃ¢u há»i tiáº¿ng Anh vá»›i 10 nhÃ£n danh má»¥c. Viá»‡c á»©ng dá»¥ng Deep Learning vÃ  Transfer Learning (BERT) Ä‘Ã£ minh chá»©ng rÃµ rá»‡t sá»± vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng truyá»n thá»‘ng.
3. HÆ°á»›ng phÃ¡t triá»ƒn Trong tÆ°Æ¡ng lai, nhÃ³m cÃ³ thá»ƒ má»Ÿ rá»™ng bÃ i toÃ¡n sang hÃ¬nh thá»©c Ä‘a ngÃ´n ngá»¯ (Multilingual) thay vÃ¬ chá»‰ tiáº¿ng Anh, Ä‘á»“ng thá»i thá»­ nghiá»‡m cÃ¡c ká»¹ thuáº­t Prompt Engineering vá»›i cÃ¡c mÃ´ hÃ¬nh GenAI má»›i nháº¥t nhÆ° GPT-3.5 hay Llama Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c gáº§n nhÆ° tuyá»‡t Ä‘á»‘i.

